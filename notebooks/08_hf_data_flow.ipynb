{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 08 - Fluxo de dados no Hugging Face (upload datasets processados)\n",
        "\n",
        "Este notebook sobe os arquivos processados (df_clean, df_log, df_capped, folds) para um dataset no HF Hub.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo criado/reusado: henriquebap/wine-ml-processed\n",
            "Base local: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed\n",
            "Repo alvo: henriquebap/wine-ml-processed\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/df_clean.csv exists: True\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/df_log.csv exists: True\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/df_capped.csv exists: True\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/df_capped_train.csv exists: True\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/df_capped_test.csv exists: True\n",
            "local file: /Users/henriquebap/Pessoal/Personal - Projects/Wine_MLProject/wine-ml-app/notebooks/data/processed/stratified_folds.json exists: True\n"
          ]
        }
      ],
      "source": [
        "# Config e imports\n",
        "import os\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi, create_repo, HfFolder\n",
        "\n",
        "# Carrega variáveis do .env\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(override=True)\n",
        "\n",
        "def get_env_strip_quotes(key: str) -> str:\n",
        "    val = os.getenv(key, '')\n",
        "    return val.strip('\\'\"')\n",
        "\n",
        "HF_TOKEN = get_env_strip_quotes('HF_TOKEN')\n",
        "HF_PROCESSED_REPO = os.getenv('HF_PROCESSED_REPO', 'henriquebap/wine-ml-processed')\n",
        "HF_PRIVATE = bool(int(os.getenv('HF_PRIVATE', '1')))\n",
        "\n",
        "if not HF_TOKEN:\n",
        "    raise ValueError('Defina HF_TOKEN no .env para autenticar no HF Hub.')\n",
        "\n",
        "# Salva token e cria API\n",
        "HfFolder.save_token(HF_TOKEN)\n",
        "api = HfApi()\n",
        "\n",
        "# Cria (ou reusa) repo\n",
        "create_repo(repo_id=HF_PROCESSED_REPO, private=HF_PRIVATE, exist_ok=True, token=HF_TOKEN, repo_type='dataset')\n",
        "print('Repo criado/reusado:', HF_PROCESSED_REPO)\n",
        "\n",
        "# Resolve base preferindo onde estão os arquivos gerados pelo EDA (notebooks/...)\n",
        "root_base = Path.cwd().parent / 'data' / 'processed'\n",
        "nb_base = Path.cwd() / 'data' / 'processed'\n",
        "# prioriza o local que contém df_capped.csv; senão usa o que existir\n",
        "candidates = [nb_base, root_base]\n",
        "base = None\n",
        "for b in candidates:\n",
        "    if (b / 'df_capped.csv').exists() or (b / 'stratified_folds.json').exists():\n",
        "        base = b\n",
        "        break\n",
        "if base is None:\n",
        "    base = nb_base if nb_base.exists() else root_base\n",
        "\n",
        "files = [\n",
        "    base / 'df_clean.csv',\n",
        "    base / 'df_log.csv',\n",
        "    base / 'df_capped.csv',\n",
        "    base / 'df_capped_train.csv',\n",
        "    base / 'df_capped_test.csv',\n",
        "    base / 'stratified_folds.json',\n",
        "]\n",
        "extra = Path.cwd().parent / 'reports' / 'eda' / 'selected_features.csv'\n",
        "if not extra.exists():\n",
        "    extra = Path.cwd() / 'reports' / 'eda' / 'selected_features.csv'\n",
        "if extra.exists():\n",
        "    files.append(extra)\n",
        "\n",
        "print('Base local:', base)\n",
        "print('Repo alvo:', HF_PROCESSED_REPO)\n",
        "for f in files:\n",
        "    print('local file:', f, 'exists:', f.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splits salvos: df_capped_train.csv, df_capped_test.csv\n"
          ]
        }
      ],
      "source": [
        "# Gera CSVs de train/test a partir dos folds\n",
        "import json\n",
        "import pandas as pd\n",
        "folds_path = base / 'stratified_folds.json'\n",
        "df_capped_path = base / 'df_capped.csv'\n",
        "if folds_path.exists() and df_capped_path.exists():\n",
        "    df = pd.read_csv(df_capped_path)\n",
        "    with open(folds_path, 'r') as f:\n",
        "        folds = json.load(f)\n",
        "    # usa fold 0 como exemplo didático\n",
        "    fold0 = folds[0]\n",
        "    tr_idx, te_idx = fold0['train_idx'], fold0['test_idx']\n",
        "    df.iloc[tr_idx].to_csv(base / 'df_capped_train.csv', index=False)\n",
        "    df.iloc[te_idx].to_csv(base / 'df_capped_test.csv', index=False)\n",
        "    print('Splits salvos: df_capped_train.csv, df_capped_test.csv')\n",
        "else:\n",
        "    print('Pulando geração de splits: arquivos não encontrados')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "staged: df_clean.csv -> raw/df_clean.csv\n",
            "staged: df_log.csv -> log/df_log.csv\n",
            "staged: df_capped.csv -> processed/full.csv\n",
            "staged: df_capped_train.csv -> processed/train.csv\n",
            "staged: df_capped_test.csv -> processed/test.csv\n",
            "staged: stratified_folds.json -> folds/stratified_folds.json\n",
            "✅ Commit enviado: 6 arquivos\n",
            "Concluído. Abra: https://huggingface.co/datasets/henriquebap/wine-ml-processed\n"
          ]
        }
      ],
      "source": [
        "# Upload batch com mapeamento para subpastas (raw/log/processed/folds/eda)\n",
        "from huggingface_hub import CommitOperationAdd\n",
        "\n",
        "HF_SPLIT = (os.getenv('HF_SPLIT') or 'all').lower()\n",
        "\n",
        "def map_path_in_repo(p: str) -> str:\n",
        "    name = Path(p).name\n",
        "    if name == 'df_clean.csv': return 'raw/df_clean.csv'\n",
        "    if name == 'df_log.csv': return 'log/df_log.csv'\n",
        "    if name == 'df_capped.csv': return 'processed/full.csv'\n",
        "    if name == 'df_capped_train.csv': return 'processed/train.csv'\n",
        "    if name == 'df_capped_test.csv': return 'processed/test.csv'\n",
        "    if name == 'stratified_folds.json': return 'folds/stratified_folds.json'\n",
        "    if name == 'selected_features.csv': return 'eda/selected_features.csv'\n",
        "    return f'other/{name}'\n",
        "\n",
        "def include_by_split(path: Path) -> bool:\n",
        "    name = path.name\n",
        "    if HF_SPLIT == 'all': return True\n",
        "    if HF_SPLIT == 'full': return name == 'df_capped.csv'\n",
        "    if HF_SPLIT == 'train': return name == 'df_capped_train.csv'\n",
        "    if HF_SPLIT == 'test': return name == 'df_capped_test.csv'\n",
        "    return True\n",
        "\n",
        "# Prepara operações de upload\n",
        "ops = []\n",
        "for path in files:\n",
        "    if path.exists() and include_by_split(path):\n",
        "        target = map_path_in_repo(str(path))\n",
        "        ops.append(CommitOperationAdd(path_in_repo=target, path_or_fileobj=str(path)))\n",
        "        print('staged:', path.name, '->', target)\n",
        "    else:\n",
        "        print('skip (missing or filtered):', path.name)\n",
        "\n",
        "# Commit batch\n",
        "if ops:\n",
        "    api.create_commit(\n",
        "        repo_id=HF_PROCESSED_REPO,\n",
        "        repo_type='dataset',\n",
        "        operations=ops,\n",
        "        token=HF_TOKEN,\n",
        "        commit_message=f\"Add {len(ops)} processed files (split={HF_SPLIT})\",\n",
        "        create_pr=False,\n",
        "    )\n",
        "    print(f'✅ Commit enviado: {len(ops)} arquivos')\n",
        "else:\n",
        "    print('Nenhum arquivo para commit')\n",
        "\n",
        "print('Concluído. Abra:', f'https://huggingface.co/datasets/{HF_PROCESSED_REPO}')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
